\begin{abstract}
    In this report, we explore optimization techniques for large-scale unconstrained problems, focusing on variations of Newton method.
    Specifically, we implement and analyze the Modified Newton Method and the Truncated Newton Method, comparing their performance on several test functions.
    Both exact and finite difference-based Hessian and gradient computations are considered. Our experiments evaluate convergence rates, computational efficiency, and the impact of preconditioning.
    Additionally, we discuss the challenges posed by finite difference approximations and the sensitivity of each method to different problem structures.
\end{abstract}